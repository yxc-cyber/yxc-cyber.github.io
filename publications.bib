@inproceedings{yang-etal-2024-arithmetic,
    title = "Arithmetic Reasoning with {LLM}: {P}rolog Generation {\&} Permutation",
    author = "Yang, Xiaocheng  and
      Chen, Bingsen  and
      Tam, Yik-Cheung",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-short.61",
    doi = "10.18653/v1/2024.naacl-short.61",
    pages = "699--710",
    abstract = "Instructing large language models (LLMs) to solve elementary school math problems has shown great success using Chain of Thought (CoT). However, the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors. We hypothesize that an LLM should focus on extracting predicates and generating symbolic formulas from the math problem description so that the underlying calculation can be done via an external code interpreter. We investigate using LLM to generate Prolog programs to solve mathematical questions. Experimental results show that our Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K benchmark across three distinct LLMs. In addition, given the insensitive ordering of predicates and symbolic formulas in Prolog, we propose to permute the ground truth predicates for more robust LLM training via data augmentation.",
}

@inproceedings{dongre-etal-2025-respact,
    title = "{R}e{S}p{A}ct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational {AI} Agents",
    author = "Dongre, Vardhan  and
      Yang, Xiaocheng  and
      Acikgoz, Emre Can  and
      Dey, Suvodip  and
      Tur, Gokhan  and
      Hakkani-Tur, Dilek",
    editor = "Torres, Maria Ines  and
      Matsuda, Yuki  and
      Callejas, Zoraida  and
      del Pozo, Arantza  and
      D'Haro, Luis Fernando",
    booktitle = "Proceedings of the 15th International Workshop on Spoken Dialogue Systems Technology",
    month = may,
    year = "2025",
    address = "Bilbao, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.iwsds-1.7/",
    pages = "72--102",
    ISBN = "979-8-89176-248-0",
    abstract = "Large language model (LLM)-based agents have been increasingly used to interact with external environments (e.g., games, APIs, etc.) and solve tasks. However, current frameworks do not enable these agents to work with users and interact with them to align on the details of their tasks and reach user-defined goals; instead, in ambiguous situations, these agents may make decisions based on assumptions. This work introduces ReSpAct (Reason, Speak, and Act), a novel framework that synergistically combines the essential skills for building task-oriented ``conversational'' agents. ReSpAct addresses this need for agents, expanding on the ReAct approach. ReSpAct framework enables agents to interpret user instructions, reason about complex tasks, execute appropriate actions and engage in dynamic dialogue to seek guidance, clarify ambiguities, understand user preferences, resolve problems, and use the intermediate feedback and responses of users to update their plans. We evaluated ReSpAct with GPT-4 in environments supporting user interaction, such as task-oriented dialogue (MultiWOZ) and interactive decision-making (Alfworld, WebShop), ReSpAct is flexible enough to incorporate dynamic user feedback and addresses prevalent issues like error propagation and agents getting stuck in reasoning loops. This results in more interpretable, human-like task-solving trajectories than baselines relying solely on reasoning traces. In two interactive decision-making benchmarks, AlfWorld and WebShop, ReSpAct outperforms strong reasoning-only method ReAct by an absolute success rate of 6{\%} and 4{\%}, respectively. In the task-oriented dialogue benchmark MultiWOZ, ReSpAct improved Inform and Success scores by 5.5{\%} and 3{\%}, respectively."
}

@inproceedings{qian-etal-2025-escapebench,
    title = "{E}scape{B}ench: Towards Advancing Creative Intelligence of Language Model Agents",
    author = "Qian, Cheng  and
      Han, Peixuan  and
      Luo, Qinyu  and
      He, Bingxiang  and
      Chen, Xiusi  and
      Zhang, Yuji  and
      Du, Hongyi  and
      Yao, Jiarui  and
      Yang, Xiaocheng  and
      Zhang, Denghui  and
      Li, Yunzhu  and
      Ji, Heng",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.39/",
    pages = "798--820",
    ISBN = "979-8-89176-251-0",
    abstract = "Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench{---}a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15{\%} average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40{\%} fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies."
}

@inproceedings{zhu-etal-2025-multiagentbench,
    title = "{M}ulti{A}gent{B}ench : Evaluating the Collaboration and Competition of {LLM} agents",
    author = "Zhu, Kunlun  and
      Du, Hongyi  and
      Hong, Zhaochen  and
      Yang, Xiaocheng  and
      Guo, Shuyi  and
      Wang, Zhe  and
      Wang, Zhenhailong  and
      Qian, Cheng  and
      Tang, Robert  and
      Ji, Heng  and
      You, Jiaxuan",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.421/",
    pages = "8580--8622",
    ISBN = "979-8-89176-251-0",
    abstract = "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents; yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, cognitive planning improves milestone achievement rates by 3{\%}. Code and dataset will be made publicly available. Code and datasets are publicavailable at https://github.com/ulab-uiuc/MARBLE"
}

@misc{bozdag2025readsystematicsurveycomputational,
      title={Must Read: A Systematic Survey of Computational Persuasion}, 
      author={Nimet Beyza Bozdag and Shuhaib Mehri and Xiaocheng Yang and Hyeonjeong Ha and Zirui Cheng and Esin Durmus and Jiaxuan You and Heng Ji and Gokhan Tur and Dilek Hakkani-Tür},
      year={2025},
      eprint={2505.07775},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.07775}, 
}

@misc{mehri2025goalalignmentllmbaseduser,
      title={Goal Alignment in LLM-Based User Simulators for Conversational AI}, 
      author={Shuhaib Mehri and Xiaocheng Yang and Takyoung Kim and Gokhan Tur and Shikib Mehri and Dilek Hakkani-Tür},
      year={2025},
      eprint={2507.20152},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.20152}, 
}

@misc{yang2025questiongenerationassessingearly,
      title={Question Generation for Assessing Early Literacy Reading Comprehension}, 
      author={Xiaocheng Yang and Sumuk Shashidhar and Dilek Hakkani-Tur},
      year={2025},
      eprint={2507.22410},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.22410}, 
}