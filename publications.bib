@inproceedings{yang-etal-2024-arithmetic,
    title = "Arithmetic Reasoning with {LLM}: {P}rolog Generation {\&} Permutation",
    author = "Yang, Xiaocheng  and
      Chen, Bingsen  and
      Tam, Yik-Cheung",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-short.61",
    doi = "10.18653/v1/2024.naacl-short.61",
    pages = "699--710",
    abstract = "Instructing large language models (LLMs) to solve elementary school math problems has shown great success using Chain of Thought (CoT). However, the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors. We hypothesize that an LLM should focus on extracting predicates and generating symbolic formulas from the math problem description so that the underlying calculation can be done via an external code interpreter. We investigate using LLM to generate Prolog programs to solve mathematical questions. Experimental results show that our Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K benchmark across three distinct LLMs. In addition, given the insensitive ordering of predicates and symbolic formulas in Prolog, we propose to permute the ground truth predicates for more robust LLM training via data augmentation.",
}

@misc{dongre2024respactharmonizingreasoningspeaking,
      title={ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents}, 
      author={Vardhan Dongre and Xiaocheng Yang and Emre Can Acikgoz and Suvodip Dey and Gokhan Tur and Dilek Hakkani-Tür},
      year={2024},
      eprint={2411.00927},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.00927}, 
}

@inproceedings{qian-etal-2025-escapebench,
    title = "{E}scape{B}ench: Towards Advancing Creative Intelligence of Language Model Agents",
    author = "Qian, Cheng  and
      Han, Peixuan  and
      Luo, Qinyu  and
      He, Bingxiang  and
      Chen, Xiusi  and
      Zhang, Yuji  and
      Du, Hongyi  and
      Yao, Jiarui  and
      Yang, Xiaocheng  and
      Zhang, Denghui  and
      Li, Yunzhu  and
      Ji, Heng",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.39/",
    pages = "798--820",
    ISBN = "979-8-89176-251-0",
    abstract = "Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench{---}a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15{\%} average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40{\%} fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies."
}

@inproceedings{zhu-etal-2025-multiagentbench,
    title = "{M}ulti{A}gent{B}ench : Evaluating the Collaboration and Competition of {LLM} agents",
    author = "Zhu, Kunlun  and
      Du, Hongyi  and
      Hong, Zhaochen  and
      Yang, Xiaocheng  and
      Guo, Shuyi  and
      Wang, Zhe  and
      Wang, Zhenhailong  and
      Qian, Cheng  and
      Tang, Robert  and
      Ji, Heng  and
      You, Jiaxuan",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.421/",
    pages = "8580--8622",
    ISBN = "979-8-89176-251-0",
    abstract = "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents; yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, cognitive planning improves milestone achievement rates by 3{\%}. Code and dataset will be made publicly available. Code and datasets are publicavailable at https://github.com/ulab-uiuc/MARBLE"
}

@misc{bozdag2025readsystematicsurveycomputational,
      title={Must Read: A Systematic Survey of Computational Persuasion}, 
      author={Nimet Beyza Bozdag and Shuhaib Mehri and Xiaocheng Yang and Hyeonjeong Ha and Zirui Cheng and Esin Durmus and Jiaxuan You and Heng Ji and Gokhan Tur and Dilek Hakkani-Tür},
      year={2025},
      eprint={2505.07775},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.07775}, 
}

@misc{mehri2025goalalignmentllmbaseduser,
      title={Goal Alignment in LLM-Based User Simulators for Conversational AI}, 
      author={Shuhaib Mehri and Xiaocheng Yang and Takyoung Kim and Gokhan Tur and Shikib Mehri and Dilek Hakkani-Tür},
      year={2025},
      eprint={2507.20152},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.20152}, 
}