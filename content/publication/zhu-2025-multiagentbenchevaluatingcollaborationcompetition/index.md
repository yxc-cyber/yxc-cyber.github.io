---
title: 'MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents'
authors:
- Kunlun Zhu
- Hongyi Du
- Zhaochen Hong
- Xiaocheng Yang
- Shuyi Guo
- Zhe Wang
- Zhenhailong Wang
- Cheng Qian
- Xiangru Tang
- Heng Ji
- Jiaxuan You
date: '2025-03-03'
publishDate: '2025-03-05T02:02:06.088951Z'
publication_types:
- article
abstract: Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%.
links:
- name: arXiv
  url: https://arxiv.org/abs/2503.01935
- name: URL
  url: https://arxiv.org/abs/2503.01935
url_code: https://github.com/MultiagentBench/MARBLE
image:
  caption: "MARBLE: showcasing interactions between task information, persona data, domain databases, memory modules, and the environment through the coordinate engine and cognitive module."
  focal_point: ''
  preview_only: false
featured: true
---
