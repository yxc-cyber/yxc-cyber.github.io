---
title: 'MultiAgentBench : Evaluating the Collaboration and Competition of LLM agents'
authors:
- Kunlun Zhu
- Hongyi Du
- Zhaochen Hong
- Xiaocheng Yang
- Shuyi Guo
- Zhe Wang
- Zhenhailong Wang
- Cheng Qian
- Robert Tang
- Heng Ji
- Jiaxuan You
date: '2025-07-01'
publishDate: '2025-08-02T03:00:21.150181Z'
publication_types:
- paper-conference
publication: '*Proceedings of the 63rd Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)*'
abstract: Large Language Models (LLMs) have shown remarkable capabilities as autonomous
  agents; yet existing benchmarks either focus on single-agent tasks or are confined
  to narrow domains, failing to capture the dynamics of multi-agent coordination and
  competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark
  designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios.
  Our framework measures not only task completion but also the quality of collaboration
  and competition using novel, milestone-based key performance indicators. Moreover,
  we evaluate various coordination protocols (including star, chain, tree, and graph
  topologies) and innovative strategies such as group discussion and cognitive planning.
  Notably, cognitive planning improves milestone achievement rates by 3%. Code and
  dataset will be made publicly available. Code and datasets are publicavailable at
  https://github.com/ulab-uiuc/MARBLE
links:
- name: arXiv
  url: https://arxiv.org/abs/2503.01935
- name: URL
  url: https://aclanthology.org/2025.acl-long.421/
url_code: https://github.com/MultiagentBench/MARBLE
image:
  caption: "MARBLE: showcasing interactions between task information, persona data, domain databases, memory modules, and the environment through the coordinate engine and cognitive module."
  focal_point: ''
  preview_only: false
featured: true
---
